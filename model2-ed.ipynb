{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10694195,"sourceType":"datasetVersion","datasetId":6626547}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-26T10:22:47.068538Z","iopub.execute_input":"2025-02-26T10:22:47.068924Z","iopub.status.idle":"2025-02-26T10:22:47.074410Z","shell.execute_reply.started":"2025-02-26T10:22:47.068893Z","shell.execute_reply":"2025-02-26T10:22:47.073390Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"input_path = \"/kaggle/input/\"\nfiles = os.listdir(input_path)\nprint(\"Files in /kaggle/input/:\", files)\n# Define dataset directory path\ndataset_path = \"/kaggle/input/gycamodel2\"\n\n# List all files in the directory\nfiles = os.listdir(dataset_path)\nprint(\"Files in gycamodel2:\", files)\n\n# Define the correct file path\nfile_path = \"/kaggle/input/gycamodel2/GygaModelRunsUganda.xlsx\"\n\n# Load the Excel file\nxls = pd.ExcelFile(file_path)\nfor sheet_name in xls.sheet_names:\n    df = xls.parse(sheet_name)\n    df.to_csv(f\"{sheet_name}.csv\", index=False)\n    print(f\"Saved {sheet_name}.csv\")\n# List all files in the specified directory\nfile_path = \"Station.csv\"  # Adjust the file path as necessary\ndf = pd.read_csv(file_path)\nprint(\"First few rows of the data:\")\nprint(df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T10:22:50.056555Z","iopub.execute_input":"2025-02-26T10:22:50.056957Z","iopub.status.idle":"2025-02-26T10:22:50.074074Z","shell.execute_reply.started":"2025-02-26T10:22:50.056925Z","shell.execute_reply":"2025-02-26T10:22:50.072540Z"}},"outputs":[{"name":"stdout","text":"Files in /kaggle/input/: []\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-7bf3abe53c7c>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# List all files in the directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Files in gycamodel2:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/gycamodel2'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/input/gycamodel2'","output_type":"error"}],"execution_count":4},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\n# Prepare X and y\nX = df.drop(columns=[\"CROP\"])  \ny = df[\"CROP\"]\n\n# Check unique values of target\nprint(\"Unique crop types:\", df[\"CROP\"].unique())\nprint(\"Crop counts:\\n\", df['CROP'].value_counts())\n\n# Identify column types\nnumerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\ncategorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n\n# Fill missing values BEFORE scaling\nX[numerical_cols] = X[numerical_cols].fillna(X[numerical_cols].median())\nX[categorical_cols] = X[categorical_cols].fillna(X[categorical_cols].mode().iloc[0])\n\n# Initialize encoders\nencoder_dict = {}\nencoder = LabelEncoder()\n\n# Encode categorical columns properly\nfor col in categorical_cols:\n    X[col] = encoder.fit_transform(X[col].astype(str))  # Ensure string conversion before encoding\n    \n    # Map original category names to encoded values\n    encoder_dict[col] = {category: index for index, category in zip(encoder.classes_, encoder.transform(encoder.classes_))}\n    \n    print(f\"Encoding for {col}: {encoder_dict[col]}\")\n\n# Encode target variable if categorical\nif y.dtype == 'object' or isinstance(y.iloc[0], str):  \n    y = encoder.fit_transform(y)\n\n# Normalize numerical columns\nscaler = StandardScaler()\nX[numerical_cols] = scaler.fit_transform(X[numerical_cols])\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Ensure the shapes of X_train, X_test, y_train, y_test\nprint(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T10:22:53.325877Z","iopub.execute_input":"2025-02-26T10:22:53.326219Z","iopub.status.idle":"2025-02-26T10:22:54.132481Z","shell.execute_reply.started":"2025-02-26T10:22:53.326186Z","shell.execute_reply":"2025-02-26T10:22:54.130948Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-66a977f1997d>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Prepare X and y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CROP\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CROP\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"],"ename":"NameError","evalue":"name 'df' is not defined","output_type":"error"}],"execution_count":5},{"cell_type":"code","source":"print(X_train_tensor.shape)  # Should output: torch.Size([32, 18])\nprint(y_train_tensor.shape)  # Should output: torch.Size([32])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T10:22:34.397550Z","iopub.status.idle":"2025-02-26T10:22:34.397977Z","shell.execute_reply":"2025-02-26T10:22:34.397842Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.metrics import mean_squared_error\nimport numpy as np\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Define Teacher Models with different architectures\nclass TeacherModel1(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(TeacherModel1, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 128)\n        self.fc2 = nn.Linear(128, output_dim)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        return self.fc2(x)\n\nclass TeacherModel2(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(TeacherModel2, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 64)\n        self.fc2 = nn.Linear(64, output_dim)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        return self.fc2(x)\n\nclass TeacherModel3(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(TeacherModel3, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 256)\n        self.fc2 = nn.Linear(256, output_dim)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        return self.fc2(x)\n\nclass TeacherModel4(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(TeacherModel4, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 512)\n        self.fc2 = nn.Linear(512, output_dim)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        return self.fc2(x)\n\n# Define Student Model\nclass StudentModel(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(StudentModel, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 128)\n        self.fc2 = nn.Linear(128, output_dim)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        return self.fc2(x)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T10:22:34.398922Z","iopub.status.idle":"2025-02-26T10:22:34.399263Z","shell.execute_reply":"2025-02-26T10:22:34.399123Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize the models\ninput_dim = X_train.shape[1]  # Number of features after preprocessing\noutput_dim = len(np.unique(y_train))  # Number of unique classes (for classification) or 1 for regression\n\nteacher_model1 = TeacherModel1(input_dim, output_dim)\nteacher_model2 = TeacherModel2(input_dim, output_dim)\nteacher_model3 = TeacherModel3(input_dim, output_dim)\nteacher_model4 = TeacherModel4(input_dim, output_dim)\nstudent_model = StudentModel(input_dim, output_dim)\n\n# Define optimizers and loss function\noptimizer_teacher1 = optim.Adam(teacher_model1.parameters(), lr=0.0001)\noptimizer_teacher2 = optim.Adam(teacher_model2.parameters(), lr=0.0001)\noptimizer_teacher3 = optim.Adam(teacher_model3.parameters(), lr=0.0001)\noptimizer_teacher4 = optim.Adam(teacher_model4.parameters(), lr=0.0001)\noptimizer_student = optim.Adam(student_model.parameters(), lr=0.0001)\ncriterion = nn.CrossEntropyLoss()  # For classification, use MSELoss for regression\n\n# Step 1: Train the teacher models\ndef train_model(model, optimizer, X_train_tensor, y_train_tensor, num_epochs=100):\n    model.train()\n    for epoch in range(num_epochs):\n        optimizer.zero_grad()\n        outputs = model(X_train_tensor)\n        loss = criterion(outputs, y_train_tensor)\n        loss.backward()\n        optimizer.step()\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T10:22:34.400130Z","iopub.status.idle":"2025-02-26T10:22:34.400537Z","shell.execute_reply":"2025-02-26T10:22:34.400392Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\n\n# Example: Define your train_loader if not already defined\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n\ndef train_student(student_model, optimizer_student, train_loader, teacher_models,\n                  temperature=3.0, alpha=0.5, num_epochs=10):\n    \n    ce_loss_fn = nn.CrossEntropyLoss()   # Standard loss for true labels\n    kd_loss_fn = nn.KLDivLoss(reduction='batchmean')  # Distillation loss\n    \n    # Ensure teacher models are in eval mode\n    for teacher_model in teacher_models:\n        teacher_model.eval()\n\n    for epoch in range(num_epochs):\n        student_model.train()  # set student model to training mode\n        running_loss = 0.0\n\n        for batch_idx, (inputs, labels) in enumerate(train_loader):\n            optimizer_student.zero_grad()\n            \n            # Student's forward pass\n            student_logits = student_model(inputs)\n            \n            # Compute standard cross-entropy loss\n            ce_loss = ce_loss_fn(student_logits, labels)\n            \n            # Compute teacher predictions for this batch and average them\n            teacher_logits_list = []\n            for teacher_model in teacher_models:\n                with torch.no_grad():\n                    teacher_logits = teacher_model(inputs)\n                    teacher_logits_list.append(teacher_logits)\n            # Average the logits from all teacher models\n            teacher_logits_avg = sum(teacher_logits_list) / len(teacher_models)\n            \n            # Soften both student and teacher predictions using the temperature\n            student_soft = F.log_softmax(student_logits / temperature, dim=1)\n            teacher_soft = F.softmax(teacher_logits_avg / temperature, dim=1)\n            \n            # Compute the distillation (KL divergence) loss\n            kd_loss = kd_loss_fn(student_soft, teacher_soft) * (temperature ** 2)\n            \n            # Combine the losses\n            loss = alpha * ce_loss + (1 - alpha) * kd_loss\n            \n            loss.backward()\n            optimizer_student.step()\n            \n            running_loss += loss.item()\n\n        avg_loss = running_loss / len(train_loader)\n        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n    \n    return student_model\n\n# Example usage:\n# Assuming you have defined student_model and teacher_model1,...,teacher_model4,\n# and created optimizer_student and train_loader:\nteacher_models = [teacher_model1, teacher_model2, teacher_model3, teacher_model4]\noptimizer_student = optim.Adam(student_model.parameters(), lr=0.001)\n\n# Train the student model using the adjusted training loop\nstudent_model = train_student(student_model, optimizer_student, train_loader,\n                              teacher_models, temperature=3.0, alpha=0.5, num_epochs=10)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T10:22:34.401417Z","iopub.status.idle":"2025-02-26T10:22:34.401866Z","shell.execute_reply":"2025-02-26T10:22:34.401678Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.optim.lr_scheduler import StepLR\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# Example: Define your train_loader if not already defined\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n\n\n\n# Convert your training data to tensors\nX_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train, dtype=torch.long)  # Use torch.float32 for regression\n\n# Train teacher models\nteacher_model1 = train_model(teacher_model1, optimizer_teacher1, X_train_tensor, y_train_tensor)\nteacher_model2 = train_model(teacher_model2, optimizer_teacher2, X_train_tensor, y_train_tensor)\nteacher_model3 = train_model(teacher_model3, optimizer_teacher3, X_train_tensor, y_train_tensor)\nteacher_model4 = train_model(teacher_model4, optimizer_teacher4, X_train_tensor, y_train_tensor)\n\n# Step 2: Get predictions from teacher models\nteacher_model1.eval()\nteacher_model2.eval()\nteacher_model3.eval()\nteacher_model4.eval()\n\nnum_epochs=10\nwith torch.no_grad():\n    teacher_preds1 = teacher_model1(X_train_tensor).cpu().numpy()\n    teacher_preds2 = teacher_model2(X_train_tensor).cpu().numpy()\n    teacher_preds3 = teacher_model3(X_train_tensor).cpu().numpy()\n    teacher_preds4 = teacher_model4(X_train_tensor).cpu().numpy()\n\n# Combine predictions from all teacher models (soft labels)\nteacher_preds_combined = (teacher_preds1 + teacher_preds2 + teacher_preds3 + teacher_preds4) / 4  # Average the predictions\n\n# Step 3: Train the student model using teacher predictions as soft labels\nstudent_model.train()\n\n# Convert test features (X_test) to a PyTorch tensor\n# Assuming X_test is a pandas DataFrame\nX_test_tensor = torch.tensor(X_test.to_numpy(), dtype=torch.float32)\ny_test_tensor = torch.tensor(y_test, dtype=torch.float32)\ny_test_tensor = y_test_tensor.long()\n\noptimizer = optim.Adam(student_model.parameters(), lr=0.001)\nscheduler = StepLR(optimizer, step_size=10, gamma=0.5)  # Adjust learning rate every 10 epochs\n\nfor epoch in range(num_epochs):\n    student_model.train()\n    for inputs, labels in train_loader:  # Using DataLoader for mini-batch training\n        optimizer.zero_grad()\n        outputs = student_model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n    scheduler.step()  # Adjust learning rate after each epoch\n\n    if epoch % 10 == 0:\n        student_model.eval()\n        with torch.no_grad():\n            # Calculate test accuracy\n            student_preds_test = student_model(X_test_tensor)\n            _, test_predicted = torch.max(student_preds_test, 1)\n            test_accuracy = np.mean(test_predicted.numpy() == y_test_tensor.numpy()) * 100\n            print(f'Epoch [{epoch}/{num_epochs}], Test Accuracy: {test_accuracy:.2f}%')\n\n            # Optionally, calculate test loss\n            test_loss = criterion(student_preds_test, y_test_tensor)\n            print(f'Epoch [{epoch}/{num_epochs}], Test Loss: {test_loss.item():.4f}')\n\n            # Calculate training accuracy for monitoring\n            student_preds_train = student_model(X_train_tensor)\n            _, train_predicted = torch.max(student_preds_train, 1)\n            train_accuracy = np.mean(train_predicted.numpy() == y_train_tensor.numpy()) * 100\n            print(f'Epoch [{epoch}/{num_epochs}], Train Accuracy: {train_accuracy:.2f}%')\n            \n            # Overfitting check\n            if train_accuracy - test_accuracy > 30:\n                print(\"Warning: The model may be overfitting!\")\n            else:\n                print(\"The model is generalizing well.\")\n\n\nstudent_model.eval()\nwith torch.no_grad():\n    student_preds = student_model(X_test_tensor).cpu().numpy()\n\n\nprint(\"Student Model Evaluation:\")\nif output_dim == 1:  # Regression task\n    mse = mean_squared_error(y_test, student_preds)\n    print(f'Mean Squared Error: {mse}')\nelse:  # Classification task\n    accuracy = np.mean(np.argmax(student_preds, axis=1) == y_test)\n    print(f'Accuracy: {accuracy*100:.2f}%')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T10:22:34.402869Z","iopub.status.idle":"2025-02-26T10:22:34.403307Z","shell.execute_reply":"2025-02-26T10:22:34.403115Z"}},"outputs":[],"execution_count":null}]}